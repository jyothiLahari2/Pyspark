{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a403b3bd-3fd5-4d0b-8387-aecd80d8ec23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+\n| id| names|         skills|\n+---+------+---------------+\n|  1|Lahari|       [de, SE]|\n|  2|JYOTHI|[DOCTER, azure]|\n+---+------+---------------+\n\nroot\n |-- id: long (nullable = true)\n |-- names: string (nullable = true)\n |-- skills: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1,'Lahari',['de','SE']),(2,'JYOTHI',['DOCTER','azure'])]\n",
    "schema = ('id','names','skills')\n",
    "\n",
    "df = spark.createDataFrame(data = data,schema = schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2560b53c-93d2-4684-ae21-dc0dcdef9f11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---------------+\n| id| names|         skills|\n+---+------+---------------+\n|  1|Lahari|       [de, SE]|\n|  2|JYOTHI|[DOCTER, azure]|\n+---+------+---------------+\n\n+---+------+---------------+------+\n| id| names|         skills| skill|\n+---+------+---------------+------+\n|  1|Lahari|       [de, SE]|    de|\n|  1|Lahari|       [de, SE]|    SE|\n|  2|JYOTHI|[DOCTER, azure]|DOCTER|\n|  2|JYOTHI|[DOCTER, azure]| azure|\n+---+------+---------------+------+\n\nroot\n |-- id: long (nullable = true)\n |-- names: string (nullable = true)\n |-- skills: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- skill: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode,col\n",
    "df.show()\n",
    "df1 = df.withColumn('skill',explode(col('skills')))\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a7b7d71-279d-4188-808a-d50cfe71401a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n| id|  name|    skills|\n+---+------+----------+\n|  1|lahari| azure,aws|\n|  2|shetty|mbbs,btech|\n+---+------+----------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- skills: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split,col\n",
    "data = [(1,'lahari','azure,aws'),(2,'shetty','mbbs,btech')]\n",
    "schema = ('id','name','skills')\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3141efde-6d20-4d4a-9c78-e59a5038b13e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n| id|  name|    skills|\n+---+------+----------+\n|  1|lahari| azure,aws|\n|  2|shetty|mbbs,btech|\n+---+------+----------+\n\n+---+------+----------+-------------+\n| id|  name|    skills|   splitarray|\n+---+------+----------+-------------+\n|  1|lahari| azure,aws| [azure, aws]|\n|  2|shetty|mbbs,btech|[mbbs, btech]|\n+---+------+----------+-------------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- skills: string (nullable = true)\n |-- splitarray: array (nullable = true)\n |    |-- element: string (containsNull = false)\n\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df1 = df.withColumn('splitarray',split(col('skills'),','))\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddea4b13-d4c4-4caa-bf28-e80fe54f8d4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+--------------+\n| id|  name|primaryskill|secondaryskill|\n+---+------+------------+--------------+\n|  1|lahari|       azure|           aws|\n|  2|shetty|      doctor|      engineer|\n+---+------+------------+--------------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- primaryskill: string (nullable = true)\n |-- secondaryskill: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array,col\n",
    "data = [(1,'lahari','azure','aws'),(2,'shetty','doctor','engineer')]\n",
    "schema = ('id','name','primaryskill','secondaryskill')\n",
    "df = spark.createDataFrame(data = data,schema = schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81eb4e0c-d6b7-4883-bd42-c15239df7cf6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+--------------+\n| id|  name|primaryskill|secondaryskill|\n+---+------+------------+--------------+\n|  1|lahari|       azure|           aws|\n|  2|shetty|      doctor|      engineer|\n+---+------+------------+--------------+\n\n+---+------+------------+--------------+------------------+\n| id|  name|primaryskill|secondaryskill|        splitarray|\n+---+------+------------+--------------+------------------+\n|  1|lahari|       azure|           aws|      [azure, aws]|\n|  2|shetty|      doctor|      engineer|[doctor, engineer]|\n+---+------+------------+--------------+------------------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- primaryskill: string (nullable = true)\n |-- secondaryskill: string (nullable = true)\n |-- splitarray: array (nullable = false)\n |    |-- element: string (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df1 = df.withColumn('splitarray',array(col('primaryskill'),col('secondaryskill')))\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcc6c327-bc83-433d-bd2b-0d8ee4c88c9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------+\n| id|  name|        skills|\n+---+------+--------------+\n|  1|lahari|  [azure, aws]|\n|  2|shetty|[mbbs, docter]|\n+---+------+--------------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- skills: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1,'lahari',['azure','aws']),(2,'shetty',['mbbs','docter'])]\n",
    "schema = ('id','name','skills')\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f38612c-155c-46b6-940e-4f5bfa9a5f6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------+\n| id|  name|        skills|\n+---+------+--------------+\n|  1|lahari|  [azure, aws]|\n|  2|shetty|[mbbs, docter]|\n+---+------+--------------+\n\n+---+------+--------------+-----------------+\n| id|  name|        skills|existvalueinarray|\n+---+------+--------------+-----------------+\n|  1|lahari|  [azure, aws]|             true|\n|  2|shetty|[mbbs, docter]|            false|\n+---+------+--------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains,col\n",
    "df.show()\n",
    "df1 = df.withColumn('existvalueinarray',array_contains(col('skills'),'azure'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbf8cb96-199e-4a68-8a78-384e3b6ad139",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------+\n| id|  name|        skills|\n+---+------+--------------+\n|  1|lahari|  [azure, aws]|\n|  2|shetty|[mbbs, docter]|\n+---+------+--------------+\n\n+---+------+--------------+-----------------+\n| id|  name|        skills|existvalueinarray|\n+---+------+--------------+-----------------+\n|  1|lahari|  [azure, aws]|            false|\n|  2|shetty|[mbbs, docter]|            false|\n+---+------+--------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains,col\n",
    "df.show()\n",
    "df1 = df.withColumn('existvalueinarray',array_contains(col('skills'),'java'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05f813af-79a9-4d5d-8943-a9408ea8c27b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "array_functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
